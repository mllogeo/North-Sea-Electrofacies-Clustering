{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* System Append to set proper path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "source": [
    "* Vaex"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.Utils import welllog\n",
    "from Source.Utils import multi_df\n",
    "from Source.Utils import well_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tqdm Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Las files Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../Data/GEOLINK_Lithology and wells NORTH SEA/'\n",
    "\n",
    "npd_wells = welllog.read_las_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las files Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of Las Files read: ' + str(len(npd_wells)))\n",
    "print('##########################')\n",
    "print('Las files ID: ' + str(npd_wells.keys()))\n",
    "print('##########################')\n",
    "print(str(npd_wells['15_9-12'].curves))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note: The Mnmonic Table above does not necessarily represent all the available log curves on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Dataframe Building and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checking unmatching unit of measurement for each log curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_mismatch_list = welllog.unit_check(npd_wells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converting all las files to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "npd_wells_df = {}\n",
    "\n",
    "for id in tqdm_notebook(list(npd_wells.keys()), desc='Converting to dataframe'):\n",
    "\n",
    "    npd_wells_df[id] = npd_wells[id].df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Filling in Log Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs_dict = welllog.log_frame(npd_wells_df, logs_list, mode='df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Main Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Creating Well ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for id in tqdm_notebook(list(npd_wells_df.keys()), desc='Adding Well Name Column'):\n",
    "\n",
    "    npd_wells_df[id]['WELL_NAME'] = id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Converting Depth to a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for id in tqdm_notebook(list(npd_wells_df.keys()), desc='Adding Depth Column'):\n",
    "\n",
    "    npd_wells_df[id]['DEPTH'] = npd_wells_df[id].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Selected Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_list = ['WELL_NAME', 'DEPTH', 'CALI', 'NPHI', 'RHOB', 'GR', 'DTC', 'RDEP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Creating Empty Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.DataFrame(columns= logs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Filling Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for id in tqdm_notebook(list(npd_wells_df.keys()), desc='Adding Depth Column'):\n",
    "\n",
    "    tmp = []\n",
    "\n",
    "    for i in range(len(logs_list)):\n",
    "\n",
    "        if logs_list[i] in npd_wells_df[id].columns:\n",
    "\n",
    "            tmp.append(logs_list[i])\n",
    "\n",
    "    df_main = df_main.append(npd_wells_df[id][tmp], ignore_index=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat_df = abs(df_main.corr()) # absolute correlation\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(corrmat_df, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mixer Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.set()\n",
    "\n",
    "sns.pairplot(df_main.dropna().sample(1000))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distribution Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * CALI Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.CALI.notnull()].CALI.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.CALI.notnull()].CALI.values); \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('CALI distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * NPHI Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.NPHI.notnull()].NPHI.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.NPHI.notnull()].NPHI.values);\n",
    "#plt.xlim(0,100) \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('NPHI distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "multiprocessing.set_start_method('spawn', True)\n",
    "\n",
    "NUM_CORES = 30\n",
    "\n",
    "df_chunks_out_count = np.array_split(df_main, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "\n",
    "    df_main = pd.concat(pool.map(multi_df.nphi_filtering, df_chunks_out_count), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * RHOB Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.RHOB.notnull()].RHOB.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.RHOB.notnull()].RHOB.values); \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('RHOB distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * GR Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.GR.notnull()].GR.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.GR.notnull()].GR.values); \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('GR distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * DTC Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.DTC.notnull()].DTC.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.DTC.notnull()].DTC.values); \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('DTC distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * RDEP Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu, sigma) = stats.norm.fit(df_main[df_main.RDEP.notnull()].RDEP.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.distplot(df_main[df_main.RDEP.notnull()].RDEP.values); \n",
    "\n",
    "plt.legend('Normal distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('RDEP distribution -- Mu:' + ' ' + str(mu) + ' ' + 'Sigma:' + ' ' + str(sigma) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relationship with the categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * CALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].CALI], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='CALI', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * NPHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].NPHI], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='NPHI', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * RHOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].RHOB], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='RHOB', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].GR], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='GR', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].DTC], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='DTC', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * RDEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "data = pd.concat([df_main[df_main.LITHOLOGY_GEOLINK.notnull()].LITHOLOGY_GEOLINK, df_main[df_main.LITHOLOGY_GEOLINK.notnull()].RDEP], axis=1)\n",
    "fig = sns.boxplot(x=\"LITHOLOGY_GEOLINK\", y='RDEP', data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outliers Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Number of outliers per row column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'Number of Outliers'\n",
    "\n",
    "df_main[val] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Non-Outlier Ranges (Expert Provided) (Used inside classify_outliers function! In here only for display purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {}\n",
    "\n",
    "ranges['CALI'] = [0, 30]\n",
    "\n",
    "ranges['NPHI'] = [0.1, 0.65] \n",
    "\n",
    "ranges['RHOB'] = [1, 4] \n",
    "\n",
    "ranges['GR'] = [0, 200]\n",
    "\n",
    "ranges['DTC'] = [40, 200]\n",
    "\n",
    "ranges['RDEP'] = [0.0001, 6000] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Filling in the Numbers of Outliers Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "multiprocessing.set_start_method('spawn', True)\n",
    "\n",
    "NUM_CORES = 7\n",
    "\n",
    "df_chunks_out_count = np.array_split(df_main, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "\n",
    "    df_main = pd.concat(pool.map(multi_df.classify_outliers, df_chunks_out_count), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Dropping Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 7\n",
    "\n",
    "df_chunks_out_count = np.array_split(df_main, NUM_CORES)\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as pool:\n",
    "\n",
    "    df_main = pd.concat(pool.map(multi_df.remove_outliers_class, df_chunks_out_count), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Null Values Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Raw Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = df_main.columns.to_list()\n",
    "\n",
    "columns.remove('DEPTH')\n",
    "\n",
    "columns.remove('WELL_NAME')\n",
    "\n",
    "columns.remove('Number of Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.barplot(x=df_main.isnull().sum().index, y=df_main.isnull().sum()) # only considering variables of interest\n",
    "plt.xticks(rotation='90')\n",
    "plt.xlabel('Variables', fontsize=10)\n",
    "plt.ylabel('Total missing values (Not a Number)', fontsize=10) # Not counting -999,25\n",
    "plt.title('Total missing values (Not a Number)', fontsize=15) # Not counting -999,25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Closer Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.barplot(x=df_main[columns].isnull().sum().index, y=df_main[columns].isnull().sum()) # only considering variables of interest\n",
    "plt.xticks(rotation='90')\n",
    "plt.xlabel('Variables', fontsize=10)\n",
    "plt.ylabel('Total missing values (Not a Number)', fontsize=10) # Not counting -999,25\n",
    "plt.title('Total missing values (Not a Number)', fontsize=15) # Not counting -999,25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Dropping Rows for Null values in Pivot Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_main.dropna(subset=['CALI', 'RHOB', 'GR', 'DTC', 'RDEP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.barplot(x=df_main.isnull().sum().index, y=df_main.isnull().sum()) # only considering variables of interest\n",
    "plt.xticks(rotation='90')\n",
    "plt.xlabel('Variables', fontsize=10)\n",
    "plt.ylabel('Total missing values (Not a Number)', fontsize=10) \n",
    "plt.title('Total missing values (Not a Number)', fontsize=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPHI Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NPHI non NULL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nphi_data = df_main[pd.notnull(df_main['NPHI'])].drop(columns=['DEPTH', 'Number of Outliers', 'WELL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nphi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nphi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat_nphi = abs(nphi_data.corr()) # absolute correlation\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(corrmat_nphi, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Using the second branch, only the three highest correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(x=\"NPHI\", y=\"CALI\", data=nphi_data.sample(100000))\n",
    "\n",
    "# Linear correlation (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(x=\"NPHI\", y=\"RHOB\", data=nphi_data.sample(100000))\n",
    "\n",
    "# negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(x=\"NPHI\", y=\"DTC\", data=nphi_data.sample(100000))\n",
    "\n",
    "# non-linear correlation (exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nphi_data[['CALI', 'DTC', 'RHOB']]\n",
    "\n",
    "Y = nphi_data['NPHI'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                * Pipelines for Models (using Robust Scaler due possible presence of Outlier -- Decrease the sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipelines = []\n",
    "\n",
    "linear_pipelines.append(('ScaledLASSO', Pipeline([('Scaler', RobustScaler()),('LASSO', linear_model.Lasso())])))\n",
    "linear_pipelines.append(('ScaledEN', Pipeline([('Scaler', RobustScaler()),('EN', linear_model.ElasticNet())])))\n",
    "linear_pipelines.append(('ScaledKNN', Pipeline([('Scaler', RobustScaler()),('KNN', KNeighborsRegressor())])))\n",
    "linear_pipelines.append(('ScaledCART', Pipeline([('Scaler', RobustScaler()),('CART', DecisionTreeRegressor())])))\n",
    "linear_pipelines.append(('ScaledGBM', Pipeline([('Scaler', RobustScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "linear_pipelines.append(('ScaledRidge', Pipeline([('Scaler', RobustScaler()),('Ridge', linear_model.Ridge())])))\n",
    "linear_pipelines.append(('ScaledOMP', Pipeline([('Scaler', RobustScaler()),('OMP', linear_model.OrthogonalMatchingPursuit())])))\n",
    "linear_pipelines.append(('ScaledBAYRID', Pipeline([('Scaler', RobustScaler()),('BAYRID', linear_model.BayesianRidge())])))\n",
    "linear_pipelines.append(('ScaledSGD', Pipeline([('Scaler', RobustScaler()),('SGD', linear_model.SGDRegressor())])))\n",
    "linear_pipelines.append(('ScaledRANDOMFOREST', Pipeline([('Scaler', RobustScaler()),('RANDOMFOREST', RandomForestRegressor(n_jobs=30))])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                * Cross-Validation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "names = []\n",
    "\n",
    "for name, model in tqdm_notebook(linear_pipelines, desc='Cross-Validation Procedure'):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "    rmse = np.sqrt(-cross_val_score(model, x_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n",
    "    results.append(rmse)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, rmse.mean(), rmse.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Hyperparemeter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Due to this time-consuming task, proximately 5 hours using a 64 core server on multiprocessing, we will only present the best set of parameters. The result of the cell bellow can be verified by the files in the model's result folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}# Number of trees in random forest\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "#rf = RandomForestRegressor(n_jobs=20)\n",
    "\n",
    "#kfold = KFold(n_splits=3, random_state=42)\n",
    "\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = kfold, verbose=10, random_state=42, n_jobs = 32, scoring='neg_mean_squared_error')\n",
    "\n",
    "#rf_random.fit(RobustScaler().fit_transform(x_train), y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_rf = RandomForestRegressor(max_depth=30, max_features='sqrt', min_samples_split=5, n_estimators=400, n_jobs=7) # the rest of the best parameters are  the default ones\n",
    "\n",
    "best_rf.fit(RobustScaler().fit_transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = best_rf.predict(RobustScaler().fit_transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print('MAE: ', str(mean_absolute_error(y_predict, y_test)), '\\n')\n",
    "print('########################', '\\n')\n",
    "print('RMSE: ', str(mean_squared_error(y_predict, y_test, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Null values Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            * Null values Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_nphi_dataset = df_main[pd.isnull(df_main['NPHI'])].drop(columns=['DEPTH', 'Number of Outliers', 'WELL_NAME'])\n",
    "\n",
    "null_nphi_dataset.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            * Features to Predict (Highest correlation with target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_predict = null_nphi_dataset[['CALI', 'DTC', 'RHOB']]\n",
    "\n",
    "features_for_predict.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            * Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nphi_prediction = best_rf.predict(RobustScaler().fit_transform(features_for_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            * Check boundary constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Maximal Range Respected: ', nphi_prediction.max() < ranges['NPHI'][1], '\\n', 'Minimal Range Respected: ', nphi_prediction.min() > ranges['NPHI'][0]) # Check if the prediction is respecting the previous established interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            * Replacing Null values for Predicted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_nphi_dataset = null_nphi_dataset\n",
    "\n",
    "predicted_nphi_dataset['NPHI'] = nphi_prediction\n",
    "\n",
    "predicted_nphi_dataset.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        * Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_df = nphi_data.append(predicted_nphi_dataset)\n",
    "\n",
    "total_df.sort_index(inplace=True)\n",
    "\n",
    "total_df['DEPTH'] = df_main['DEPTH'].values\n",
    "\n",
    "total_df['WELL_NAME'] = df_main['WELL_NAME'].values\n",
    "\n",
    "total_df.head(n=5)"
   ]
  },
  {
   "source": [
    "# Checkpoint 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_file = '../Data/total.csv.gz'\n",
    "\n",
    "# total_df.to_csv(path_file,index=False, compression='gzip')"
   ]
  },
  {
   "source": [
    "* Importing libraries for checkpoint 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "import lasio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "\n",
    "from Source.Utils import welllog\n",
    "from Source.Utils import multi_df\n",
    "from Source.Utils import well_plot\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('../Data/total.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing the clustering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_dict = {}\n",
    "\n",
    "wells = total_df['WELL_NAME'].unique().tolist()\n",
    "\n",
    "for well in tqdm_notebook(wells, desc='Process Progress'):\n",
    "\n",
    "    GR = total_df[total_df['WELL_NAME'] == well]['GR'].values\n",
    "\n",
    "    RHOB = total_df[total_df['WELL_NAME'] == well]['RHOB'].values\n",
    "\n",
    "    NPHI = total_df[total_df['WELL_NAME'] == well]['NPHI'].values\n",
    "\n",
    "    DTC = total_df[total_df['WELL_NAME'] == well]['DTC'].values\n",
    "\n",
    "    RDEP = total_df[total_df['WELL_NAME'] == well]['RDEP'].values\n",
    "\n",
    "    listafinal = np.concatenate((GR, RHOB, NPHI, DTC, RDEP))\n",
    "\n",
    "    tmp_dict[well] = listafinal\n",
    "\n",
    "df_clust = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in tmp_dict.items() ]))"
   ]
  },
  {
   "source": [
    "* Filtering Wells with High percentage of Null values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clust.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_count = df_clust.isnull().sum()\n",
    "\n",
    "tresh = int(np.percentile(null_count, 50))\n",
    "\n",
    "df_clust_clean = df_clust.copy()\n",
    "\n",
    "for well in tqdm_notebook(df_clust.columns, desc='Process Progress'):\n",
    "    if null_count[well] > tresh:\n",
    "        df_clust_clean.drop(columns=well, inplace=True)\n"
   ]
  },
  {
   "source": [
    "* Filling Null values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_clean.fillna(df_clust_clean.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_clean.isnull().sum().max()"
   ]
  },
  {
   "source": [
    "* Importing the geographic locations of the drilled wells"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_exploration = pd.read_csv('../Data/wellbore_exploration_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_exploration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_well = well_exploration[['wlbWellboreName','wlbNsUtm', 'wlbEwUtm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_well.head()"
   ]
  },
  {
   "source": [
    "* Transposing the dataframe to prepare for the clustering algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_t = df_clust_clean.T"
   ]
  },
  {
   "source": [
    "* Filtering well Northing and Easting coordinates and appending to the clusterized dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_wells = df_clust_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_t['UTM-N'] = 0.0\n",
    "df_clust_t['UTM-E'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for well_name in tqdm_notebook(list_of_wells, desc='Process Progress'):\n",
    "    if well_name in list(geo_well['wlbWellboreName'].values):\n",
    "        df_clust_t.loc[well_name, 'UTM-N'] = geo_well[geo_well['wlbWellboreName'] == well_name]['wlbNsUtm'].values\n",
    "        df_clust_t.loc[well_name, 'UTM-E'] = geo_well[geo_well['wlbWellboreName'] == well_name]['wlbEwUtm'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_t[['UTM-N', 'UTM-E']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_t = df_clust_t[df_clust_t['UTM-N'] != 0]"
   ]
  },
  {
   "source": [
    "* Normalizing columns "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust_norm = df_clust_t.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_clust_norm = MinMaxScaler().fit_transform(df_clust_norm)\n",
    "\n",
    "df_clust_norm = pd.DataFrame(df_clust_norm, index=df_clust_t.index, columns=df_clust_t.columns)"
   ]
  },
  {
   "source": [
    "* Importing K-means algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "source": [
    "* Evaluating the optimum number of clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcss = [] # Within cluster sum of squares to analyze k-means performance\n",
    "\n",
    "k_number_clusters = np.arange(1, 11) # define number of clusters to test\n",
    "\n",
    "for k in tqdm_notebook(k_number_clusters, desc='K-Means Hyperparameter Tunning'): \n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=42, max_iter=500, n_jobs=9) # k-means model definition\n",
    "\n",
    "    kmeans.fit(df_clust_t) # fitting to our dataframe\n",
    "\n",
    "    wcss.append(kmeans.inertia_) # appending intertia value to our list for further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))    \n",
    "plt.plot(k_number_clusters, wcss, linewidth=2, color=\"red\", marker =\"8\")\n",
    "plt.xlabel(\"K Clusters Value\")\n",
    "plt.xticks(np.arange(1,11,1))\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title('K-Means Elbow Plot Evaluation Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_clustering = KMeans(n_clusters=N_CLUSTERS, init=\"k-means++\", random_state=42, max_iter=500, n_jobs=9)\n",
    "\n",
    "df_clust_t['Cluster'] = optimum_clustering.fit_predict(df_clust_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))    \n",
    "\n",
    "sns.scatterplot(data=df_clust_t, x=\"UTM-E\", y=\"UTM-N\", palette='bright', hue=\"Cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}